{
 "metadata": {
  "name": "",
  "signature": "sha256:2156bddee14d8dff91a9610b506273096e16e8232b81dfd29044d22f881848ef"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import urllib\n",
      "import urllib2\n",
      "import os\n",
      "import gzip\n",
      "import StringIO\n",
      "import glob\n",
      "import re\n",
      "from zipfile import ZipFile\n",
      "from bz2 import BZ2File\n",
      "import xml.etree.ElementTree as ET"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# English Corpus\n",
      "# Project Gutenberg\n",
      "# http://mirrors.pglaf.org/gutenberg-iso/pgdvd072006.iso\n",
      "def make_english_corpus():\n",
      "    bytes_written = 0\n",
      "    with open(\"corpus_english.txt\", 'wb') as outfile:\n",
      "        for root, dirnames, filenames in os.walk('pgdvd072006'):\n",
      "            print str(outfile.tell()) + \" bytes written to corpus_english.txt\"\n",
      "            for fname in filenames:\n",
      "                if fname.endswith(\".zip\"):\n",
      "                    z = ZipFile(os.path.join(root, fname), 'r')\n",
      "                    if len(z.namelist()) > 0 :\n",
      "                        bookfile = z.namelist()[0]\n",
      "                        if bookfile.endswith(\".txt\"):\n",
      "                            t = z.read(bookfile)\n",
      "                            i = t.find(\"*END*\")\n",
      "                            if i >= 0:\n",
      "                                t = t[i + 5:]\n",
      "                                t = t[t.find(\"*END*\") + 5:]\n",
      "                            outfile.write(t)\n",
      "                if outfile.tell() > 1024 * 1024 * 1024 * 1.2: #Limit to 1.2G\n",
      "                    return"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Chinese Wikipedia\n",
      "def download_chinese_corpus():\n",
      "    ch_file = \"http://dumps.wikimedia.org/zhwiki/20141105/zhwiki-20141105-pages-articles.xml.bz2\"\n",
      "    fname = os.path.basename(ch_file)\n",
      "    urllib.URLopener().retrieve(ch_file, fname)\n",
      "\n",
      "def make_chinese_corpus(do_download=True, do_uncompress=True):\n",
      "    if do_download:\n",
      "        download_chinese_corpus()\n",
      "    \n",
      "    xmlfilename = \"zhwiki-20141105-pages-articles.xml\"\n",
      "    if do_uncompress:\n",
      "        with open(xmlfilename, 'wb') as xmlfile:\n",
      "            bz = BZ2File(\"zhwiki-20141105-pages-articles.xml.bz2\")\n",
      "            t = bz.read(1024 * 1024)\n",
      "            while len(t) > 0:\n",
      "                xmlfile.write(t)\n",
      "                t = bz.read(1024 * 1024)\n",
      "                \n",
      "    outfilename = \"corpus_chinese.txt\"\n",
      "    with open(outfilename, 'wb') as outfile:\n",
      "        for event, elem in ET.iterparse(\"zhwiki-20141105-pages-articles.xml\", events=('start', 'end', 'start-ns', 'end-ns')):\n",
      "            if event != \"end\":\n",
      "                continue\n",
      "            if not elem.tag.endswith(\"page\"):\n",
      "                continue\n",
      "            t = elem.find(\"{http://www.mediawiki.org/xml/export-0.9/}revision\").find(\"{http://www.mediawiki.org/xml/export-0.9/}text\").text\n",
      "            if t != None:\n",
      "                outfile.write(t.encode('UTF-8'))\n",
      "            if outfile.tell() > 1024 * 1024 * 1024 * 1.2: #Limit to 1.2G\n",
      "                    return"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Genomic Corpus\n",
      "# Human Reference Genome - Chromosomes 1 - 6\n",
      "def make_genomic_corpus():\n",
      "    files = [\n",
      "        \"ftp://ftp.ncbi.nlm.nih.gov/genomes/H_sapiens/Assembled_chromosomes/seq/hs_ref_GRCh38_chr1.fa.gz\",\n",
      "        \"ftp://ftp.ncbi.nlm.nih.gov/genomes/H_sapiens/Assembled_chromosomes/seq/hs_ref_GRCh38_chr2.fa.gz\",\n",
      "        \"ftp://ftp.ncbi.nlm.nih.gov/genomes/H_sapiens/Assembled_chromosomes/seq/hs_ref_GRCh38_chr3.fa.gz\",\n",
      "        \"ftp://ftp.ncbi.nlm.nih.gov/genomes/H_sapiens/Assembled_chromosomes/seq/hs_ref_GRCh38_chr4.fa.gz\",\n",
      "        \"ftp://ftp.ncbi.nlm.nih.gov/genomes/H_sapiens/Assembled_chromosomes/seq/hs_ref_GRCh38_chr5.fa.gz\",\n",
      "        \"ftp://ftp.ncbi.nlm.nih.gov/genomes/H_sapiens/Assembled_chromosomes/seq/hs_ref_GRCh38_chr6.fa.gz\"\n",
      "    ]\n",
      "    outfilename = \"corpus_genomic.txt\"\n",
      "    with open(outfilename, 'wb') as outfile:\n",
      "        for ch_file in files:\n",
      "            print \"Downloading \" + ch_file\n",
      "            fname = os.path.basename(ch_file)\n",
      "            urllib.URLopener().retrieve(ch_file, fname)\n",
      "            buf = StringIO.StringIO()\n",
      "            print \"Decompressing \" + fname\n",
      "            with gzip.open(fname, 'rb') as infile:\n",
      "                buf.write(infile.read())\n",
      "            print \"Appending \" + fname + \" to \" + outfilename\n",
      "            outfile.write(re.sub(\"[^ACGT]\", \"\", buf.getvalue()))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "make_english_corpus()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0 bytes written to corpus_english.txt\n",
        "0 bytes written to corpus_english.txt\n",
        "0 bytes written to corpus_english.txt\n",
        "0 bytes written to corpus_english.txt\n",
        "0 bytes written to corpus_english.txt\n",
        "0 bytes written to corpus_english.txt\n",
        "0 bytes written to corpus_english.txt\n",
        "0 bytes written to corpus_english.txt\n",
        "0 bytes written to corpus_english.txt\n",
        "0 bytes written to corpus_english.txt\n",
        "0 bytes written to corpus_english.txt\n",
        "0 bytes written to corpus_english.txt\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "make_chinese_corpus(do_download=False, do_uncompress=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "make_genomic_corpus()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Downloading ftp://ftp.ncbi.nlm.nih.gov/genomes/H_sapiens/Assembled_chromosomes/seq/hs_ref_GRCh38_chr1.fa.gz\n",
        "Decompressing hs_ref_GRCh38_chr1.fa.gz"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Appending hs_ref_GRCh38_chr1.fa.gz to corpus_genomic.txt"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Downloading ftp://ftp.ncbi.nlm.nih.gov/genomes/H_sapiens/Assembled_chromosomes/seq/hs_ref_GRCh38_chr2.fa.gz"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Decompressing hs_ref_GRCh38_chr2.fa.gz"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Appending hs_ref_GRCh38_chr2.fa.gz to corpus_genomic.txt"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Downloading ftp://ftp.ncbi.nlm.nih.gov/genomes/H_sapiens/Assembled_chromosomes/seq/hs_ref_GRCh38_chr3.fa.gz"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Decompressing hs_ref_GRCh38_chr3.fa.gz"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Appending hs_ref_GRCh38_chr3.fa.gz to corpus_genomic.txt"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Downloading ftp://ftp.ncbi.nlm.nih.gov/genomes/H_sapiens/Assembled_chromosomes/seq/hs_ref_GRCh38_chr4.fa.gz"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Decompressing hs_ref_GRCh38_chr4.fa.gz"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Appending hs_ref_GRCh38_chr4.fa.gz to corpus_genomic.txt"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Downloading ftp://ftp.ncbi.nlm.nih.gov/genomes/H_sapiens/Assembled_chromosomes/seq/hs_ref_GRCh38_chr5.fa.gz"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Decompressing hs_ref_GRCh38_chr5.fa.gz"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Appending hs_ref_GRCh38_chr5.fa.gz to corpus_genomic.txt"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Downloading ftp://ftp.ncbi.nlm.nih.gov/genomes/H_sapiens/Assembled_chromosomes/seq/hs_ref_GRCh38_chr6.fa.gz"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Decompressing hs_ref_GRCh38_chr6.fa.gz"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Appending hs_ref_GRCh38_chr6.fa.gz to corpus_genomic.txt"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}